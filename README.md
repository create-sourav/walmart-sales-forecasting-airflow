# ğŸ¬ Walmart Sales Forecasting â€” End-to-End MLOps Pipeline

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/Docker-20.10+-blue.svg)](https://www.docker.com/)
[![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.0+-red.svg)](https://airflow.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

An end-to-end MLOps pipeline for forecasting Walmart weekly sales using Machine Learning, Docker, and Apache Airflow. This project demonstrates production-grade ML workflows with proper orchestration, containerization, and reproducibility.

## ğŸ“Œ Project Overview

This project implements a complete MLOps pipeline that goes beyond model accuracy to showcase:

- **Structured project layout** following industry best practices
- **Reproducible training** using Docker containerization
- **Automated ML pipelines** orchestrated with Apache Airflow
- **Proper handling** of data, models, and experiments
- **Production-ready** design suitable for interviews and real-world applications

## ğŸ¯ Business Problem

Walmart operates across multiple stores with fluctuating weekly sales influenced by various factors:

- ğŸ‰ **Holidays** â€” Special events impact shopping behavior
- â›½ **Fuel Price** â€” Transportation costs affect consumer spending
- ğŸ“Š **CPI** â€” Consumer Price Index reflects inflation
- ğŸ’¼ **Unemployment** â€” Job market conditions influence purchasing power
- ğŸŒ¦ï¸ **Seasonal Patterns** â€” Weather and time of year drive demand

**Objective:** Predict weekly sales accurately using historical data and time-series-aware feature engineering.

## ğŸ§  Solution Approach

### 1ï¸âƒ£ Data Understanding

- **Dataset Source:** [Kaggle â€“ Walmart Sales Forecasting](https://www.kaggle.com/)
- **Observations:** Weekly sales per store with economic indicators
- **Time-Series Nature:** Data is preserved chronologically (no random shuffling)

### 2ï¸âƒ£ Feature Engineering

Implemented time-series aware features to capture temporal patterns:

**Temporal Features:**
- Year, Month, Week extraction

**Lag Features:**
- `Weekly_Sales_lag_1` â€” Previous week's sales
- `Weekly_Sales_lag_2` â€” Sales from two weeks ago

**Rolling Statistics:**
- `Rolling_mean_4` â€” 4-week moving average
- `Rolling_mean_12` â€” 12-week moving average

**Categorical Encoding:**
- One-Hot Encoding for Store identifiers

**Scaling:**
- StandardScaler for numerical features

### 3ï¸âƒ£ Model Training & Evaluation

Multiple models were trained and compared:

| Model | Description |
|-------|-------------|
| **Linear Regression** | Baseline model |
| **Random Forest Regressor** | Ensemble method with decision trees |
| **Gradient Boosting Regressor** âœ… | Best performing model |

**Evaluation Metrics:**
- **RMSE** (Root Mean Squared Error)
- **MAE** (Mean Absolute Error)
- **RÂ² Score** (Coefficient of Determination)

### ğŸ“Š Model Performance

| Model | RMSE | MAE | RÂ² |
|-------|------|-----|-----|
| Gradient Boosting âœ… | Low | Low | High |
| Random Forest | Moderate | Moderate | High |
| Linear Regression | High | High | Low |

> **Note:** Metrics are evaluated using time-based train/test split, not random split, to respect the temporal nature of the data.

**Winner:** Gradient Boosting Regressor achieved the best balance of accuracy and generalization.

## âš™ï¸ MLOps Architecture

### ğŸ” Pipeline Automation (Airflow)

The entire ML workflow is orchestrated using Apache Airflow:

- **DAG Name:** `walmart_ml_pipeline`
- **Pipeline Stages:**
  1. **Preprocess Data** â€” Feature engineering and transformation
  2. **Train Model** â€” Model training and comparison
  3. **Test Model** â€” Evaluation and prediction generation

Each stage is an isolated, repeatable task ensuring modularity and maintainability.

### ğŸ³ Containerization (Docker)

- All dependencies are containerized for consistency
- Ensures reproducibility across different machines
- Separate Docker setup for:
  - **ML Execution** â€” Model training and inference
  - **Airflow Orchestration** â€” Workflow management

## ğŸ“‚ Project Structure

```
walmart_sales/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ walmart_ml_pipeline.py   # Airflow DAG definition
â”‚   â”œâ”€â”€ Dockerfile                   # Custom Airflow image
â”‚   â””â”€â”€ docker-compose.yml           # Airflow orchestration setup
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ inspect_data.py              # Data inspection utilities
â”‚   â”œâ”€â”€ preprocess_data.py           # Feature engineering pipeline
â”‚   â”œâ”€â”€ train.py                     # Model training script
â”‚   â”œâ”€â”€ test.py                      # Model evaluation script
â”‚   â””â”€â”€ future_forecast.py           # Future predictions generator
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ walmart_sales.csv            # Raw dataset
â”‚   â”œâ”€â”€ processed_walmart_sales.csv  # Processed features
â”‚   â””â”€â”€ future_sales_forecast.csv    # Forecast output
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ Gradient_Boosting.pkl        # Trained GB model
â”‚   â”œâ”€â”€ Random_Forest.pkl            # Trained RF model
â”‚   â”œâ”€â”€ Linear_Regression.pkl        # Trained LR model
â”‚   â””â”€â”€ model_performance_metrics.csv # Model comparison results
â”‚
â”œâ”€â”€ Dockerfile                        # ML container definition
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ .gitignore                        # Git ignore rules
â””â”€â”€ README.md                         # Project documentation
```

## ğŸš€ Getting Started

### Prerequisites

- Docker (20.10+)
- Docker Compose
- Git

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/create-sourav/walmart-sales-forecasting-airflow.git
cd walmart-sales-forecasting-airflow
```

### 2ï¸âƒ£ Build & Run Airflow

```bash
cd airflow
docker compose up -d
```

### 3ï¸âƒ£ Access Airflow UI

Open your browser and navigate to:

```
http://localhost:8080
```

### 4ï¸âƒ£ Trigger the ML Pipeline

1. In the Airflow UI, locate the `walmart_ml_pipeline` DAG
2. Enable the DAG using the toggle switch
3. Click **"Trigger DAG"** to start the pipeline
4. Monitor the execution of each task:
   - âœ… `preprocess_data`
   - âœ… `train_model`
   - âœ… `test_model`

### ğŸ” Airflow Credentials

When using `airflow standalone`, credentials are auto-generated.

To view them:

```bash
docker logs airflow
```

## ğŸ”„ Handling New Incoming Data (Incremental Retraining)

This pipeline is designed to support new incoming data without breaking the existing workflow.

### ğŸ“¥ Scenario

Walmart receives new weekly sales data (for new dates or stores) periodically.

**Example:**
- New week's sales CSV arrives every Sunday
- Data structure remains the same as `walmart_sales.csv`

### ğŸ§© How New Data Is Incorporated

#### 1ï¸âƒ£ Append New Data (Not Replace)

New data should be appended to the existing raw dataset:

**ğŸ“‚ Location:**
```
data/walmart_sales.csv
```

**Process:**
- Add new rows at the bottom
- Do not modify historical records
- Maintain the same schema (columns)

**Example Schema:**
```
Store | Date | Weekly_Sales | Holiday_Flag | Temperature | Fuel_Price | CPI | Unemployment
```

#### 2ï¸âƒ£ Preprocessing Automatically Handles New Data

The `preprocess_data.py` script is idempotent and time-aware:

**What it does every run:**
- Reads the entire updated dataset
- Sorts by `Store` and `Date`
- Recomputes:
  - Lag features
  - Rolling averages
  - Time features
- Drops invalid rows caused by lag creation
- Saves a fresh processed dataset

**ğŸ“„ Output:**
```
data/processed_walmart_sales.csv
```

â¡ï¸ **No manual feature updates needed when new data arrives.**

#### 3ï¸âƒ£ Time-Based Training Preserves Order

The training script uses time-based splitting to ensure newer data is always used for testing:

```python
split = int(len(df) * 0.8)
train = df.iloc[:split]
test  = df.iloc[split:]
```

This ensures:
- **Older data** â†’ Training
- **Newer data** â†’ Validation/Testing
- **No data leakage** between train and test sets

#### 4ï¸âƒ£ Model Retraining Is Automatic via Airflow

Once new data is added:

1. **Trigger the DAG manually** OR
2. **Let the scheduled DAG run** execute automatically

**Airflow automatically:**
1. Preprocesses updated data
2. Retrains all models
3. Evaluates performance
4. Saves updated models

**ğŸ“‚ Updated models overwrite older versions:**
```
models/gradient_boosting_model.pkl
```

This ensures:
- Always using the latest trained model
- No stale predictions

### ğŸ” Typical Production Flow

```
New Data Arrives
    â†“
Raw CSV Updated
    â†“
Airflow DAG Triggered
    â†“
Preprocessing
    â†“
Model Retraining
    â†“
Model Testing
    â†“
Updated Model Saved
```

### ğŸ§  Why This Design Is MLOps-Correct

- âœ… **No manual intervention** â€” Fully automated pipeline
- âœ… **No feature leakage** â€” Time-based splitting
- âœ… **Fully reproducible** â€” Idempotent preprocessing
- âœ… **Scalable** â€” Works with weekly/monthly updates
- âœ… **Industry standard** â€” Batch-based retraining approach

## ğŸ§ª Key MLOps Concepts Demonstrated

- âœ… **Time-Series Aware ML** â€” Proper handling of temporal data
- âœ… **Feature Engineering Best Practices** â€” Lag features and rolling statistics
- âœ… **Model Comparison & Evaluation** â€” Systematic model selection
- âœ… **Reproducible ML using Docker** â€” Containerized environments
- âœ… **Workflow Orchestration with Airflow** â€” Automated pipelines
- âœ… **Production-Style Project Structure** â€” Industry-standard organization
- âœ… **Incremental Retraining Support** â€” Handles new data seamlessly
- âœ… **GitHub-Ready MLOps Repository** â€” Professional presentation

## ğŸ“Œ Important Notes

- Large model files should ideally be stored using **Git LFS** or a model registry (e.g., **MLflow**)
- This project focuses on **MLOps workflow design**, not just model accuracy
- Dataset files are kept intentionally for learning and demonstration purposes

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Sourav Mondal**  
MBA (Business Analytics)  
Aspiring Data Scientist / MLOps Engineer

- GitHub: [@create-sourav](https://github.com/create-sourav)
- LinkedIn: [Connect with me](https://www.linkedin.com/in/sourav-mondal)

## â­ Show Your Support

If you find this project useful, please consider giving it a â­ on GitHub â€” it helps a lot!

---

**Built with â¤ï¸ for the Data Science and MLOps community**
